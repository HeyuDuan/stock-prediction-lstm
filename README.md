LSTM Stock Price Prediction Project
# Project Overview
This project is an independent practice I developed following my interview with the Hong Kong University of Science and Technology (HKUST) on November 4th. During the interview, I recognized gaps in my understanding of deep learning network-related concepts, so I designed this stock price prediction project centered on Long Short-Term Memory (LSTM) networks to systematically master the structure, training logic, and engineering implementation of recurrent neural networks (RNNs). The core goal of the project is to predict short-term stock prices using historical time series data, while directly addressing the knowledge shortcomings I identified during the HKUST interview.
# Technical Stack
To build this project, I used a targeted technical stack aligned with the project’s needs: TensorFlow 2.10.0 serves as the deep learning framework (with the LSTM recurrent neural network as the core model), while NumPy 1.21.6, Pandas 2.0.3, and Scikit-learn 1.3.0 handle data processing tasks like normalization, dataset splitting, and sequence generation. For visualization of training progress and results, I employed Matplotlib 3.7.2 and Seaborn 0.12.2, and Flask 2.0.3 provides a foundation for potential web deployment of the model later.
# Core Implementation
The core implementation focuses on addressing the gradient vanishing issue common in traditional RNNs: I designed a multi-layer LSTM architecture with regularization mechanisms, including a first LSTM layer (50 neurons, return sequences enabled) paired with batch normalization and 20% Dropout, followed by a second LSTM layer (50 neurons) with the same regularization components, and a final dense output layer (1 neuron) for price prediction. For training, I used the Adam optimizer (initial learning rate 0.001) with mean squared error (MSE) as the loss function; to avoid overfitting and optimize convergence, I implemented early stopping (patience=10, which halted training at Epoch 43) and ReduceLROnPlateau (patience=5, for adaptive learning rate decay). Before training, I preprocessed historical stock data (handling normalization and missing values), split it into a 70% training set, 15% validation set, and 15% test set, and generated time series sequences (time step=60) to fit the LSTM’s input requirements.
 # Technical Challenges & Solutions
During development, I encountered two key technical challenges: first, version conflicts between TensorFlow and dependent packages (e.g., NumPy, Pandas, Scipy) led to DLL loading failures and automatic uninstallation of critical libraries. To resolve this, I specified compatible versions for all packages (e.g., TensorFlow 2.10.0 paired with NumPy 1.21.6), installed all dependencies in a single command to prevent pip’s automatic version replacement, and exported a requirements.txt file to lock the stable environment. Second, initial model overfitting was mitigated by combining early stopping and Dropout layers to balance fitting and generalization.
 # Experimental Results
The project’s experimental results demonstrate strong performance: the model achieved a mean squared error (MSE) of 0.073071 and mean absolute error (MAE) of 0.006719 on the test set. The best-performing model (saved at Epoch 43) is stored at models/lstm_model.keras, and training progress (including training/validation loss, MAE, and learning rate curves) is visualized in static/images/training_history.png—a figure that reflects the model’s stable convergence.
 # Reflection & Future Work
Reflecting on this project, it represents targeted improvement after my HKUST interview: through building the LSTM network from scratch, I mastered the core principles of recurrent neural networks (especially LSTM’s gate mechanism), filled the gaps in my network-related knowledge from the interview, and gained practical experience solving engineering issues like package version conflicts. Looking ahead, I plan to optimize the model by integrating multi-dimensional features (e.g., trading volume, macroeconomic indicators), test advanced architectures (e.g., BiLSTM, Transformer), deploy it as a web service via Flask, and—if given the opportunity—deepen my study of deep learning network theories in HKUST’s courses to apply them to more complex financial time series scenarios.
 # Environment Setup
To replicate this project’s environment: first, create and activate a virtual environment (Anaconda is recommended) with conda create -n tf_stable python=3.9 and conda activate tf_stable; then install dependencies using pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple; finally, run the project with python main.py. The requirements.txt file includes all locked versions: tensorflow==2.10.0, numpy==1.21.6, pandas==2.0.3, scikit-learn==1.3.0, matplotlib==3.7.2, seaborn==0.12.2, scipy==1.7.3, flask==2.0.3, and python-dotenv==1.0.0.