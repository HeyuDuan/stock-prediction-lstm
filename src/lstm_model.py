# src/lstm_model.py
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import (
    EarlyStopping, 
    ReduceLROnPlateau, 
    ModelCheckpoint,
    TensorBoard
)
import matplotlib.pyplot as plt
import os
from datetime import datetime

class LSTMModel:
    """LSTMè‚¡ç¥¨é¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, input_shape, model_path='models/lstm_model.keras'):
        self.input_shape = input_shape
        self.model_path = model_path
        self.model = None
        self.history = None
        
    def build_model(self, lstm_units=[50, 50], dropout_rate=0.2, learning_rate=0.001):
        """æ„å»ºLSTMæ¨¡å‹æ¶æ„"""
        print("ğŸ—ï¸ æ„å»ºLSTMæ¨¡å‹...")
        
        model = Sequential([
            # ç¬¬ä¸€å±‚LSTM
            LSTM(
                units=lstm_units[0],
                return_sequences=True,
                input_shape=self.input_shape,
                kernel_initializer='glorot_uniform',
                recurrent_initializer='orthogonal'
            ),
            BatchNormalization(),
            Dropout(dropout_rate),
            
            # ç¬¬äºŒå±‚LSTM
            LSTM(
                units=lstm_units[1],
                return_sequences=False,
                kernel_initializer='glorot_uniform',
                recurrent_initializer='orthogonal'
            ),
            BatchNormalization(),
            Dropout(dropout_rate),
            
            # å…¨è¿æ¥å±‚
            Dense(25, activation='relu', kernel_initializer='he_normal'),
            Dense(1, kernel_initializer='glorot_uniform')  # è¾“å‡ºå±‚
        ])
        
        # ç¼–è¯‘æ¨¡å‹
        optimizer = Adam(
            learning_rate=learning_rate,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-07
        )
        
        model.compile(
            optimizer=optimizer,
            loss='mse',  # å‡æ–¹è¯¯å·®
            metrics=['mae', 'mse']  # å¹³å‡ç»å¯¹è¯¯å·®å’Œå‡æ–¹è¯¯å·®
        )
        
        self.model = model
        self._print_model_summary()
        
        return model
    
    def _print_model_summary(self):
        """æ‰“å°æ¨¡å‹æ‘˜è¦"""
        print("=" * 60)
        print("ğŸ“Š æ¨¡å‹æ¶æ„æ‘˜è¦")
        print("=" * 60)
        self.model.summary()
        
        # è®¡ç®—æ€»å‚æ•°
        trainable_params = np.sum([tf.keras.backend.count_params(w) for w in self.model.trainable_weights])
        non_trainable_params = np.sum([tf.keras.backend.count_params(w) for w in self.model.non_trainable_weights])
        
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"ä¸å¯è®­ç»ƒå‚æ•°: {non_trainable_params:,}")
        print(f"æ€»å‚æ•°: {trainable_params + non_trainable_params:,}")
        print("=" * 60)
    
    def train(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):
        """è®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = f"logs/fit/{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        
        # å®šä¹‰å›è°ƒå‡½æ•°
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=15,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-6,
                verbose=1
            ),
            ModelCheckpoint(
                filepath=self.model_path,
                monitor='val_loss',
                save_best_only=True,
                verbose=1
            ),
            TensorBoard(log_dir=log_dir, histogram_freq=1)
        ]
        
        # è®­ç»ƒæ¨¡å‹
        self.history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        print(f"âœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {self.model_path}")
        
        return self.history
    
    def evaluate(self, X_test, y_test):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        if self.model is None:
            self.load_model()
        
        print("ğŸ“ˆ è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        evaluation = self.model.evaluate(X_test, y_test, verbose=0)
        
        # æ‰“å°ç»“æœ
        print("=" * 60)
        print("ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ")
        print("=" * 60)
        print(f"æµ‹è¯•æŸå¤± (MSE): {evaluation[1]:.6f}")
        print(f"æµ‹è¯•MAE: {evaluation[2]:.6f}")
        print("=" * 60)
        
        return evaluation
    
    def predict(self, X):
        """è¿›è¡Œé¢„æµ‹"""
        if self.model is None:
            self.load_model()
        
        return self.model.predict(X, verbose=0)
    
    def save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        self.model.save(self.model_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {self.model_path}")
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        self.model = tf.keras.models.load_model(self.model_path)
        print(f"âœ… æ¨¡å‹å·²ä» {self.model_path} åŠ è½½")
        return self.model